# 02:40
Can't sleep. Time to do something useful.

https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#planning-a-cluster  
I'll be hosting my own high availability cluster.
For now I only want to run k8s, but I can see a future where I contribute to
it.

https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#managing-a-cluster  
https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#securing-a-cluster  
https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#optional-cluster-services  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/certificates/#creating-certificates  
https://kubernetes.io/docs/concepts/cluster-administration/certificates/#easyrsa  
https://kubernetes.io/docs/concepts/cluster-administration/certificates/#openssl  
https://kubernetes.io/docs/concepts/cluster-administration/certificates/#cfssl  
This is review. I know about all of these tools. I'm going to use cfssl.

https://kubernetes.io/docs/concepts/cluster-administration/certificates/#distributing-self-signed-ca-certificate  
https://kubernetes.io/docs/concepts/cluster-administration/certificates/#certificates-api  
Will read about these in the task section.

# 02:45
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#load-balancers  
Neat to see you can configure a lot about an ELB using k8s.

https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#cloudconf  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#typical-configuration  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#global  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#load-balancer  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#block-storage  
https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#router  
Don't need any of this.

# 02:50
https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/  
App deployed. Exposed via a Service. Now what?

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#organizing-resource-configurations  
Suggested grouping of resource types together for a given application into
a single file. Resources are created in the order they are specified and are
separated by `---`.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#bulk-operations-in-kubectl  
Some neat tricks for managing resources en masse.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively  
Use the following labels:
app: <name>
tier: <backend/frontend>
role: <master/slave>
track: <stable/canary>

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments  
If your service omits track label it will serve traffic to stable & canary
at the same time.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-labels  
https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-annotations  
Showing how to implicitly change labels / annotations. No. Will use source
controlled config files for that.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#scaling-your-application  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#in-place-updates-of-resources  
https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-apply  
High level description of how apply manages patching resources. This is
review.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-edit  
Open a config in editor and send back on save. Nope.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-patch  
Make patch calls directly from the command line. Nope.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#disruptive-updates  
Using `kubectl replace path/to/config.yml --force` deletes and recreates.

https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-your-application-without-a-service-outage  
Make sense to me. This is review a this point.

# 03:05
https://kubernetes.io/docs/concepts/cluster-administration/networking/  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#summary  
K8s assumes all Pod can communicate regardless of which Node they are on.
Each Pod has its own IP. No container/host port mapping needed. Pods make
it possible to treat a group of containers like a VM with respect to port
allocation, configuration, etc.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#docker-model  
Docker uses host-private networking. Bridge, called docker0 is created.
For each container, docker allocates veth and connects it to the bridge.
veth is mapped to eth0 in the container. The in-container network gets an
ip in the range of the bridge. Containers in this model can only talk if
the are attached to the same bridge (machine).

Communicating across node is done via careful port forwarding.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#kubernetes-model  
Port coordination is hard with large teams. Also, developers shouldn't have
to think about cluster level concerns. Dynamic port allocation is complex.
Every app has to use port flags, api servers have to insert dynamic ports
into config blocks, Services have to find each-other, etc etc. K8s sidesteps
this and mandates that:

All containers can communicate with all other containers without NAT.
All nodes can communicate with all containers without NAT.
The IP a container sees itself as is the same as what others see.

This makes it easy to port apps that used to run on VMs with their own IP
and expect to communicate with other VMs etc.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this  
So, you can't just have two computers running docker, slap k8s on it and
call it a day...

https://kubernetes.io/docs/concepts/cluster-administration/networking/#cilium  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#contiv  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#contrail  
OSS projects making this sort of networking possible.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel  
Simple overlay network that works.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#google-compute-engine-gce  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#kube-router  
Handles inter-pod communication with LVS/IPVS. No network overlays.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#l2-networks-and-linux-bridging  
Neat.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#multus-a-multi-network-plugin  
Wow, that is a lot of acronyms I have never seen. Some day I will take that
rabbit hole to a whole dimension of networking stuff.

https://kubernetes.io/docs/concepts/cluster-administration/networking/#nsx-t  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#nuage-networks-vcs-virtualized-cloud-services  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#openvswitch  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#ovn-open-virtual-networking  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#project-calico  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#romana  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#weave-net-from-weaveworks  
https://kubernetes.io/docs/concepts/cluster-administration/networking/#cni-genie-from-huawei  
Hokay... there are lot of these.

# 03:30
Detour to read the networking design proposal.

https://github.com/kubernetes/community/blob/master/contributors/design-proposals/network/networking.md  
Neat.

https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/  
Skipping, this is alpha stuff.

https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/  
Docs about how to run device plugins on nodes.

https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/  
https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#what-is-a-sysctl  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#namespaced-vs-node-level-sysctls  
Use taints and tolerations to control what is scheduled on nodes with custom
sysctls.

https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#safe-vs-unsafe-sysctls  
There is a very small subset of sysctls that can be used in an isolated way
from Pod to Pod.

https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#enabling-unsafe-sysctls  
There is an escape hatch to using any sysctls for performance tuning.

https://kubernetes.io/docs/concepts/cluster-administration/sysctl-cluster/#setting-sysctls-for-a-pod  
Sysctls can be controlled via Pod specification but is in beta.

# 03:45
https://kubernetes.io/docs/concepts/cluster-administration/logging/  
Logs have a different lifecycle independent of Nodes, Pods or containers.
K8s doesn't provide a built-in solution for managing this back end.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#basic-logging-in-kubernetes  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#logging-at-the-node-level  
Kubelet keeps one terminated container around for inspection.
Consider using logrotate at the Node level.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#system-component-logs  
The scheduler itself and kube-proxy run in containers.
Kubelet and container runtime log to systemd on Node.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#cluster-level-logging-architectures  
Logging approaches:
- One logging agent per node
- Dedicated "sidecar" container for logging
- Pushing logs directly to aggregator from application

https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-node-logging-agent  
Logging agent per node is commonly a DaemonSet. This is the encouraged
approach. It's limited to capturing stderr/stdout, though.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-sidecar-container-with-the-logging-agent  
https://kubernetes.io/docs/concepts/cluster-administration/logging/#streaming-sidecar-container  
Neaaaaat.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#sidecar-container-with-a-logging-agent  
Putting a logging agent in place with every application can be done. It may
consume a lot of resources or make it harder to see logs with `kubectl logs`
though.

https://kubernetes.io/docs/concepts/cluster-administration/logging/#exposing-logs-directly-from-the-application  
Pushing from every application is possible, sure.

# 04:10
https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/  
This is a feature that removes unused containers and images.

https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#image-collection  
K8s manages all images through "imageManager"

https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#container-collection  
https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#user-configuration  
Some flags for kubelet to further control this:

image-gc-high-threshold
percentage of disk usage when gc is run.

image-gc-low-threshold
percentage of free space kubelet tries to maintain, default 80%.

minimum-container-ttl-duration
minimum age before GC-ing a container is even considered.

maximum-dead-containers-per-container
max number of dead containers a given pod is allowed.

maximum-dead-containers
max number of dead containers period at the node level.

https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/#deprecation  
The notes I just took about kubelet flags may be obviated by new options.

# 04:20
https://kubernetes.io/docs/concepts/cluster-administration/federation/#why-federation  
Run multiple clusters across multiple clouds. Primarily supported by making
it possible to sync config between clouds and enable cross-cluster discovery
via automatically configured DNS.

Could also support super paranoid HA and the avoidance of cloud provider
lock in.

Having clusters globally located can lower latency. Fault isolation may be
possible... one cluster fails and another still runs. Scalability for very
large clusters can be an issue that running multiple clusters helps.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#caveats  
Increased network/bandwidth cost... a control plane is monitoring all of the
clusters to ensure the state is correct. This can generate a lot of traffic.

The federation control plane can break all clusters.

Federation project is relatively new.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#hybrid-cloud-capabilities  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#api-resources  
All links to the tasks section. Skipping for now.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#cascading-deletion  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#scope-of-a-single-cluster  
Suggested to keep all VMs in a single AZ to make reasoning about resources
easier.

https://kubernetes.io/docs/concepts/cluster-administration/federation/#selecting-the-right-number-of-clusters  
Makes sense to me.

# 04:35
https://kubernetes.io/docs/concepts/cluster-administration/proxies/  
https://kubernetes.io/docs/concepts/cluster-administration/proxies/#requesting-redirects  
Makes sense to me.

https://kubernetes.io/docs/concepts/cluster-administration/controller-metrics/  
Check.

https://kubernetes.io/docs/concepts/policy/resource-quotas/  
Ensure teams sharing a cluster don't hog resources...
Usually quotes are enforced by setting constraints on a namespace.

https://kubernetes.io/docs/concepts/policy/pod-security-policy/#what-is-a-pod-security-policy  
Pod Security Policy enforces cluster-level controls on what pods can and
can't do.

https://kubernetes.io/docs/concepts/policy/pod-security-policy/#runasuser  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#selinux  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#supplementalgroups  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#fsgroup  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#controlling-volumes  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#host-network  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#allowprivilegeescalation  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#defaultallowprivilegeescalation  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#allowedhostpaths  
Check.

https://kubernetes.io/docs/concepts/policy/pod-security-policy/#admission  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#creating-a-pod-security-policy  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#getting-a-list-of-pod-security-policies  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#editing-a-pod-security-policy  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#deleting-a-pod-security-policy  
https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies  
Check.

https://kubernetes.io/docs/concepts/policy/pod-security-policy/#working-with-rbac  
Check.

# 04:45
Finished with the Concepts section. Sweet. Feeling a bit more tired now.

# 10:10
Here's where I left off when I detoured into "Concepts"

https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/  
https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#before-you-begin  
Check.

https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deploy-mysql  
Deployed, but failed on getting the PersistentVolumeClaim.

‚ùØ k describe pvc mysql-pv-claim
....
Events:
Type    Reason         Age                From                         Message
----    ------         ----               ----                         -------
Normal  FailedBinding  11s (x10 over 2m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set

Looking up how to handle volumes with minikube now...

# 10:30
Pausing to help Tara with Adobe Premier Pro.

# 11:00
Back to it.

Okay, got it working by adding a hostPath Persistent Volume so there is
something for the deployment to actually claim.

https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#accessing-the-mysql-instance  
Check.
Note to self, using clusterIP: None on services that don't have replicas is
a good idea because DNS will just resolve right to the pod.

https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#updating  
When dealing with state, don't scale without StatefulSet or things will go
awry with persistence. If the volume can be mounted by more than one Pod I
would assume we'd have two mysql instances running trying to use the same
data files.

Use the strategy "recreate" instead of "rolling" or the same issues above
could happen when running an update.

https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/#deleting-a-deployment  
Makes sense to me.

# 11:25
Breaking for food and client emails.

# 11:50
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/  
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#before-you-begin  
Makes sense to me.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#deploy-mysql  
Makes sense to me.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#services  
Making a headless service for the StatefulSet to hold DNS entries. We do
this because we don't want load balancing? In this mode DNS will return
IPs for all the pods... not quite sure. Continuing on.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#statefulset  
Neat usage of initContainers. I sort of dislike that all the commands are
locked up in yml files instead of shell scripts.

Minikube lost network connectivity again.
...debugging.

# 12:45
Well, there goes an hour. Finally figured out what was going on. Any time I
connect to a VPN networking for xhyve gets fubar.
Note to self: figure out how to fix that.

Moving on.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#understanding-stateful-pod-initialization  
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#generating-configuration  
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#cloning-existing-data  
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#starting-replication  
All makes sense to me.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#sending-client-traffic  
Neat.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#simulating-pod-and-node-downtime  
https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#break-the-readiness-probe  
Works.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#delete-pods  
Deleting a pod works... k8s fixes it.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#drain-a-node  
This doesn't work because I only have one node. I get the general concept
though.

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#scaling-the-number-of-slaves  
Scaling up a StatefulSet creates more persistent volume claims and scaling
down does NOT remove them (to make scaling up faster later).

https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/#cleaning-up  
Makes sense to me.

# 13:15
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/  
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#before-you-begin  
Check.

https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#use-a-strategic-merge-patch-to-update-a-deployment  
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch  
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#use-a-json-merge-patch-to-update-a-deployment  
https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#alternate-forms-of-the-kubectl-patch-command  
All makes sense to me.

https://kubernetes.io/docs/tasks/run-application/upgrade-pet-set-to-stateful-set/  
Not relevant to me, skipping.

https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/  
https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/  
These are covered by the mysql example. Skipping.

https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/  
https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#before-you-begin  
https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#statefulset-considerations  
https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#delete-pods  
https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/#force-deletion  
Makes sense to me.

# 13:20
Client comms.

# 13:50
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#overview  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#passing-a-configuration-file  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#examples  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#updating-the-container-image  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#examples-1  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#required-and-optional-fields  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#walkthrough  
https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/#troubleshooting  
This is all review. Skimmed, nothing of note.

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/  

This document is missing an overview/table of contents. Making a PR:
https://github.com/kubernetes/website/pull/6298  

What is Horizontal Pod Autoscaling?
Automatically ramping up/down replicas based on observed metrics.

How does the Horizontal Pod Autoscaler work?
Get metrics, scale based on settings.

API Object
Support for Horizontal Pod Autoscaler in kubectl
Makes sense to me.

Autoscaling during rolling update
HPA only binds to a deployment object. You can't use the underlying
replication controller.

Support for cooldown/delay
Prevent pod thrashing by adding delays for up and downscaling. Makes sense
to me.

Support for multiple metrics
Support for custom metrics
Makes sense to me.

# 14:00
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/  

This document is missing an overview/table of contents. Making a PR:
https://github.com/kubernetes/website/pull/6299  

Prerequisites
Step One: Run & expose php-apache server
Step Two: Create Horizontal Pod Autoscaler
Step Three: Increase load
Is a while loop curl call really going to generate load?
Yup. It works! Neat.

Step Four: Stop load
All makes sense to me.

Autoscaling on multiple metrics and custom metrics
Appendix: Horizontal Pod Autoscaler Status Conditions
All makes sense to me.
This is so slick.

Creating the autoscaler from a .yaml file
Example yaml file, looks good. On to the next thing.

Pretty sweet. Just watched autoscaling work on my laptop 0_0.

# 14:20
https://kubernetes.io/docs/tasks/run-application/configure-pdb/  
https://kubernetes.io/docs/tasks/run-application/configure-pdb/#before-you-begin  
Check. I don't think I'll need this feature so I'm reading with a bit less
intensity.

https://kubernetes.io/docs/tasks/run-application/configure-pdb/#protecting-an-application-with-a-poddisruptionbudget  
https://kubernetes.io/docs/tasks/run-application/configure-pdb/#identify-an-application-to-protect  
Makes sense to me.

https://kubernetes.io/docs/tasks/run-application/configure-pdb/#think-about-how-your-application-reacts-to-disruptions  
Makes sense to me.

https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget  
Makes sense to me.

https://kubernetes.io/docs/tasks/run-application/configure-pdb/#create-the-pdb-object  
https://kubernetes.io/docs/tasks/run-application/configure-pdb/#check-the-status-of-the-pdb  
Review of basic kubectl commands.

https://kubernetes.io/docs/tasks/run-application/configure-pdb/#arbitrary-controllers-and-selectors  
Makes sense to me.

# 14:25
https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#example-multiple-job-objects-from-template-expansion  
https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#basic-template-expansion  
https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#multiple-template-parameters  
https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/#alternatives  
This is an example of how to do bash templating or jinja2 templating.

# 14:30
Signing out for the day to focus on client work.
